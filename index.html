<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jin-Seop Lee</title>

  <meta name="author" content="Jin-Seop Lee">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:73%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Jin-Seop Lee
                  </p>
                  <p><br>
                    I am a Ph.D. candidate in the <a href="https://iislab.skku.edu/iish/">Information & Intelligence System Lab (IISLab)</a> at Sungkyunkwan University, supervised by Prof. <a href="https://iislab.skku.edu/iish/">Jee-Hyong Lee</a>.

                    I received my Bachelor's degree from Sungkyunkwan University.

<!--                    My research interest lies in the areas of computer vision, deep learning, and machine learning.-->
<!--                    Currently, I am interested in designing efficient neural fields architecture.-->
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:wlstjq0602@skku.edu">Email</a> &nbsp;/&nbsp;
                    <a href="https://jinsuby.github.io/data/CV_Jin-Seop_Lee.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=6oAkrhMAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/lee-jin-seop-664172154/">LinkedIn</a> &nbsp;/&nbsp;
                    <a href="https://github.com/jinsuby">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="./data/lee.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 20%;"
                      alt="profile photo" src="./data/lee.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>News</h2>
                  <p style="line-height: 1.8;">
                    <b>[2025-07]</b> Two papers accepted to BMVC 2025.<br>
                    <b>[2025-05]</b> One paper accepted to ACL 2025.<br>
                    <b>[2024-12]</b> One paper accepted to AAAI 2025.<br>
                    <b>[2024-10]</b> One paper accepted to WACV 2025.<br>
                    <b>[2024-07]</b> Two papers accepted to ECCV 2024.<br>
                    <b>[2024-02]</b> One paper accepted to CVPR 2024.<br>
                    <b>[2024-01]</b> One paper accepted to EAAI 2024.<br>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>



          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                            My research interests include various tasks in computer vision, deep learning, and machine learning.
                            I have been conducted research on data-efficient learning methods, such as self-/semi-supervised learning, weakly-supervised learning, and domain generalization, etc.
                            I am currently conducting research to improve the understanding and reasoning abilities of multi-modal large language models (MLLMs).
                            Also, I am interested in multi-modal learning, video understanding, image/video synthesis, efficient AI systems, and streaming video LLMs.
<!--                        My research interests include various tasks in computer vision, multi-modal learning, deep learning, and machine learning.-->
<!--                        I have been conducted research on data-efficient learning methods, including semi-/self-supervised learning, weakly-supervised learning, and domain generalization.-->
<!--                        Furthermore, I am currently interested in multi-modal learning, efficient reasoning model, and video understanding/synthesis.-->
<!--                    My research interest lies in the areas of computer vision, graphics, and machine learning. Currently, I am interested in designing efficient neural fields architecture.-->
<!--                    Representative papers are <span style="background-color:#e1eafb;">highlighted</span>.-->
                  </p>
                </td>
              </tr>
            </tbody>
          </table>



          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='data/CountCluster.PNG' width="160"
                      style='margin-top: 30px';>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://jinsuby.github.io/">
                    <span class="papertitle">CountCluster: Training-Free Object Quantity Guidance with Cross-Attention Map Clustering for Text-to-Image Generation</span>
                  </a>
                  <br>
                    Joohyeon Lee, <b>Jin-Seop Lee</b>, Jee-Hyong Lee
                  <br>
                  <em>Preprint</em>
                  <br>
                  <div>
                    <a href="https://github.com/JoohyeonL22/CountCluster/">[Project page]</a>
<!--                    <a href="https://arxiv.org/abs/2412.09074">[Paper]</a>-->
                    <a href="https://github.com/JoohyeonL22/CountCluster/">[Code]</a>
                    <a href="https://arxiv.org/abs/2508.10710">[arXiv]</a>
                  </div>
<!--                  <p>-->
<!--                      TAG effectively captures the temporal context of videos and addresses distorted similarity distributions without training.</p>-->
                </td>
              </tr>



              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='data/BMVC2025_TAG.png' width="160"
                      style='margin-top: 30px';>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://jinsuby.github.io/">
                    <span class="papertitle">TAG: A Simple Yet Effective Temporal-Aware Approach for Zero-Shot Video Temporal Grounding</span>
                  </a>
                  <br>
                    <b>Jin-Seop Lee*</b>, Sungjoon Lee*, Jaehan Ahn, Yunseok Choi, Jee-Hyong Lee
                  <br>
                  <em>BMVC</em>, 2025
                  <br>
                  <div>
                    <a href="https://jinsuby.github.io/">[Project page]</a>
<!--                    <a href="https://arxiv.org/abs/2412.09074">[Paper]</a>-->
                    <a href="https://jinsuby.github.io/">[Code]</a>
                    <a href="https://jinsuby.github.io/">[arXiv]</a>
                  </div>
                  <p>
                      TAG effectively captures the temporal context of videos and addresses distorted similarity distributions without training.</p>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='data/BMVC_2025_OSTTA.png' width="160"
                      style='margin-top: 30px';>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://jinsuby.github.io/">
                    <span class="papertitle">Stabilizing Open-Set Test-Time Adaptation via Primary-Auxiliary Filtering and Knowledge-Integrated Prediction</span>
                  </a>
                  <br>
                    Byung-Joon Lee, <b>Jin-Seop Lee</b>, Jee-Hyong Lee
                  <br>
                  <em>BMVC</em>, 2025
                  <br>
                  <div>
                    <a href="https://jinsuby.github.io/">[Project page]</a>
<!--                    <a href="https://arxiv.org/abs/2412.09074">[Paper]</a>-->
                    <a href="https://jinsuby.github.io/">[Code]</a>
                    <a href="https://jinsuby.github.io/">[arXiv]</a>
                  </div>
                  <p>
                    OSTTA employs an auxiliary filter to validate data and calibrates the outputs of the adapting model, EMA model, and source model to integrate their complementary knowledge.</p>
                </td>
              </tr>



              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='data/ACL2025_DCG_SQL.png' width="160"
                      style='margin-top: 30px' ;>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://jinsuby.github.io/dcg-sql/">
                    <span class="papertitle">DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep Contextual Schema Link Graph</span>
                  </a>
                  <br>
                    Jihyung Lee*, <b>Jin-Seop Lee*</b>, Jaehoon Lee, YunSeok Choi, Jee-Hyong Lee
                  <br>
                  <em>ACL</em>, 2025
                  <br>
                  <div>
                    <a href="https://jinsuby.github.io/dcg-sql/">[Project page]</a>
<!--                    <a href="https://arxiv.org/abs/2412.09074">[Paper]</a>-->
                    <a href="https://github.com/jjklle/DCG-SQL">[Code]</a>
                    <a href="https://arxiv.org/abs/2505.19956">[arXiv]</a>
                  </div>
                  <p>
                    DCG-SQL improves text-to-SQL generation by incorporating a deep contextual schema link graph that captures key elements and semantic relationships between the question and database schema.
                  </p>
                </td>
              </tr>


              <tr onmouseout="dyc3dgs_stop()" onmouseover="dyc3dgs_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='data/AAAI2025_domclp.png' width="160"
                      style='margin-top: 30px' ;>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://jinsuby.github.io/domclp/">
                    <span class="papertitle">DomCLP: Domain-wise Contrastive Learning with Prototype Mixup for Unsupervised Domain Generalization</span>
                  </a>
                  <br>
                  <b>Jin-Seop Lee</b>, Noo-ri Kim, Jee-Hyong Lee
                  <br>
                  <em>AAAI</em>, 2025
                  <br>
                  <div>
                    <a href="https://jinsuby.github.io/domclp/">[Project page]</a>
                    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/33993">[paper]</a>
                    <a href="https://github.com/JINSUBY/DomCLP">[Code]</a>
                    <a href="https://arxiv.org/abs/2412.09074">[arXiv]</a>
                  </div>
                  <p>
                    DomCLP enhances generalization to unseen domains by combining domain-aware contrastive learning with prototype mixup to learn robust and domain-invariant features.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='data/WACV2025_fsae.png' width="160"
                      style='margin-top: 30px' ;>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://openaccess.thecvf.com/content/WACV2025/papers/Choi_Feature-Level_and_Spatial-Level_Activation_Expansion_for_Weakly-Supervised_Semantic_Segmentation_WACV_2025_paper.pdf">
                    <span class="papertitle">Feature-level and Spatial-level Activation Expansion for Weakly-Supervised Semantic Segmentation</span>
                  </a>
                  <br>
                  Junsu Choi*, <b>Jin-Seop Lee*</b>, Noo-ri Kim, SuHyun Yoon, Jee-Hyong Lee
                  <br>
                  <em>WACV</em>, 2025
                  <br>
                  <div>
<!--                    <a href="https://xiangyu1sun.github.io/Factorize-3DGS/">[Project page]</a>-->
                    <a href="https://openaccess.thecvf.com/content/WACV2025/papers/Choi_Feature-Level_and_Spatial-Level_Activation_Expansion_for_Weakly-Supervised_Semantic_Segmentation_WACV_2025_paper.pdf">[Paper]</a>
                    <a href="https://github.com/obeychoi0120/FSAE">[Code]</a>
                  </div>
                  <p>
                    FSAE improves weakly-supervised segmentation by expanding Class Activation Maps along feature and spatial dimensions to better capture full object regions.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='data/ECCV2024_ignore.png' width="160"
                      style='margin-top: 30px' ;>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11377.pdf">
                    <span class="papertitle">IGNORE: Information Gap-based False Negative Loss Rejection for Single Positive Multi-Label Learning</span>
                  </a>
                  <br>
                  Gyeong Ryeol Song, Noo-ri Kim, <b>Jin-Seop Lee</b>, Jee-Hyong Lee
                  <br>
                  <em>ECCV</em>, 2024
                  <br>
                  <div>
<!--                    <a href="https://tae-mo.github.io/crad/">[Project page]</a>-->
                    <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11377.pdf">[Paper]</a>
                    <a href="https://github.com/gyeong-ryeol-song/SPML-IGNORE">[Code]</a>
                  </div>
                  <p>
                    IGNORE identifies and filters hidden positive labels using information gaps from pseudo masks to reduce false negatives in single-positive multi-label learning.
                  </p>
                </td>
              </tr>

              <tr onmouseout="c3dgs_stop()" onmouseover="c3dgs_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='data/ECCV2024_exmatch.png' width="160" style='margin-top: 15px' ;>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11377.pdf">
                    <span class="papertitle">ExMatch: Semi-Supervised Learning with Scarce Labeled Samples with Additional Exploitation of Unlabeled Samples</span>
                  </a>
                  <br>
                  Noo-ri Kim, <b>Jin-Seop Lee</b>, Jee-Hyong Lee
                  <br>
                  <em>ECCV</em>, 2024
                  <br>
                  <div>
<!--                    <a href="https://maincold2.github.io/c3dgs/">[Project page]</a>-->
                    <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11377.pdf">[Paper]</a>
<!--                    <a href="https://github.com/maincold2/Compact-3DGS">[Code]</a>-->
                  </div>
                  <p>
                    ExMatch boosts semi-supervised learning with extremely limited labels by selectively leveraging confident unlabeled samples for self-training.
                  </p>
                </td>
              </tr>

              <!-- <tr onmouseout="cam_stop()" onmouseover="cam_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='cam_image'><video  width=100% height=100% muted autoplay loop>
            <source src="https://maincold2.github.io/c3dgs/videos/ours.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <data src='c3dgs/images/thumbnail.png' width="160" style='margin-top: 35px;'>
          </div>
          <script type="text/javascript">
            function cam_start() {
              document.getElementById('cam_image').style.opacity = "1";
            }

            function cam_stop() {
              document.getElementById('cam_image').style.opacity = "0";
            }
            cam_stop()
          </script>
        </td> -->

              <tr>
                  <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='data/CVPR2024_lnl.png' width="160" style='margin-top: 15px' ;>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Learning_with_Structural_Labels_for_Learning_with_Noisy_Labels_CVPR_2024_paper.pdf">
                    <span class="papertitle">Learning with Structural Labels in Learning with Noisy Labels</span>
                  </a>
                  <br>
                  Noo-ri Kim*, <b>Jin-Seop Lee*</b>, Jee-Hyong Lee
                  <br>
                  <em>CVPR</em>, 2024
                  <br>
                  <div>
<!--                    <a href="https://maincold2.github.io/cam/">[Project page]</a>-->
                    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Learning_with_Structural_Labels_for_Learning_with_Noisy_Labels_CVPR_2024_paper.pdf">[Paper]</a>
<!--                    <a href="https://github.com/maincold2/cam">[Code]</a>-->
                  </div>
                  <p>
                    LSL improves noisy label learning by incorporating structural information from data distribution to prevent overfitting to incorrect labels.
                  </p>
                </td>
              </tr>

              <tr onmouseout="ffnerv_stop()" onmouseover="ffnerv_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='data/EAAI2024_zigzag.png' width="160" style='margin-top: 30px' ;>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://www.sciencedirect.com/science/article/pii/S0952197623014677">
                    <span class="papertitle">Automation of Trimming Die Design Inspection by Zigzag Process Between AI and CAD Domains</span>
                  </a>
                  <br>
                  <b>Jin-Seop Lee*</b>, Tae-Hyun Kim*, Sang-Hwan Jeon, Sung-Hyun Park, Sang-Hi Kim, Eun-Ho Lee, Jee-Hyong Lee
                  <br>
                  <em>EAAI(Engineering Applications of Artificial Intelligence)</em>, 2024, TOP 3%, IF 8.0
                  <br>
                  <div>
                    <a href="https://www.sciencedirect.com/science/article/pii/S0952197623014677">[Paper]</a>
                  </div>
                  <p>
                    Our zigzag process automates trimming die inspection through alternating AI and CAD collaboration, achieving high accuracy and significantly reduced inspection time.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img
                      src='data/IEEE2023_defect.png'
                      width="160" style='margin-top: 15px' ;>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/10129970">
                    <span class="papertitle">Automatic defect classification using semi-supervised learning with defect localization</span>
                  </a>
                  <br>
                  Yusung Kim, <b>Jin-Seop Lee</b>, Jee-Hyong Lee
                  <br>
                  <em>IEEE Transactions on Semiconductor Manufacturing</em>, 2023
                  <br>
                  <div>
                    <a href="https://ieeexplore.ieee.org/abstract/document/10129970">[Paper]</a>
                  </div>
                  <p>
                    Our method achieves robust defect classification in semiconductor manufacturing by combining localization-guided detection with semi-supervised learning.
                  </p>
                </td>
              </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:30px;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Education</h2>
                    <p>
                      <li><b>Sungkyunkwan University (SKKU), South Korea</b></li>
                      &nbsp &nbsp &nbsp &nbsp Integrated M.S. and Ph.D., Artificial Intelligence </br>
                      &nbsp &nbsp &nbsp &nbsp Mar. 2021 - present
                    </p>
                    <p>
                      <li><b>Sungkyunkwan University (SKKU), South Korea</b></li>
                      &nbsp &nbsp &nbsp &nbsp B.S., Mechanical Engineering / Computer Engineering (Double Major) </br>
                      &nbsp &nbsp &nbsp &nbsp Mar. 2015 - Feb. 2021
                    </p>
                    <p>
                      <li><b>Incheon Science High School, South Korea</b></li>
                      &nbsp &nbsp &nbsp &nbsp Mar. 2013 - Feb. 2015
                    </p>
                </td>
              </tr>
            </tbody>
          </table>

<!--              <tr>-->
<!--                <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--                  <div class="one">-->
<!--                    <data-->
<!--                      src='https://iris.skku.edu/publication/j20_tomm_2022/featured_hu8654dd4afe97dd5cbdc405565a8d43f2_324538_720x2500_fit_q75_h2_lanczos.webp'-->
<!--                      width="160" style='margin-top: 40px' ;>-->
<!--                  </div>-->
<!--                </td>-->
<!--                <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--                  <a href="https://dl.acm.org/doi/abs/10.1145/3551389">-->
<!--                    <span class="papertitle">Scalable Color Quantization for Task-Centric Image Compression</span>-->
<!--                  </a>-->
<!--                  <br>-->
<!--                  Jae Hyun Park, Sang Hoon Kim, <b>Joo Chan Lee</b>, Jong Hwan Ko-->
<!--                  <br>-->
<!--                  <em>ACM TOMM</em>, 2022-->
<!--                  <br>-->
<!--                  <div>-->
<!--                    <a href="https://dl.acm.org/doi/abs/10.1145/3551389">[Paper]</a>-->
<!--                  </div>-->
<!--                  <br>-->
<!--                  <p>-->
<!--                    Images with variable color space sizes can be extracted from a master image generated by a single-->
<!--                    DNN model.-->
<!--                    <br>-->
<!--                </td>-->
<!--              </tr>-->


<!--              <tr>-->
<!--                <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--                  <div class="one">-->
<!--                    <data-->
<!--                      src='https://iris.skku.edu/publication/c30_avss_2021/Featured_hu6aef2193e6bece41bd2a0e680fa624e7_139860_720x2500_fit_q75_h2_lanczos.webp'-->
<!--                      width="160" style='margin-top: 25px' ;>-->
<!--                  </div>-->
<!--                </td>-->
<!--                <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--                  <a href="https://ieeexplore.ieee.org/document/9663806">-->
<!--                    <span class="papertitle">A Splittable DNN-Based Object Detector for Edge-Cloud Collaborative-->
<!--                      Real-Time Video Inference</span>-->
<!--                  </a>-->
<!--                  <br>-->
<!--                  <b>Joo Chan Lee</b>, Yongwoo Kim, SungTae Moon, Jong Hwan Ko-->
<!--                  <br>-->
<!--                  <em>AVSS</em>, 2021-->
<!--                  <br>-->
<!--                  <div>-->
<!--                    <a href="https://ieeexplore.ieee.org/document/9663806">[Paper]</a>-->
<!--                  </div>-->
<!--                  <br>-->
<!--                  <p>-->
<!--                    A splittable object detector for real-time collaborative inference.-->
<!--                    <br>-->
<!--                </td>-->
<!--              </tr>-->

<!--              <tr>-->
<!--                <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--                  <div class="one">-->
<!--                    <data-->
<!--                      src='https://iris.skku.edu/publication/j12_iet_el_2021/featured_hu6aef2193e6bece41bd2a0e680fa624e7_229612_720x2500_fit_q75_h2_lanczos.webp'-->
<!--                      width="160" style='margin-top: 35px' ;>-->
<!--                  </div>-->
<!--                </td>-->
<!--                <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--                  <a href="https://dl.acm.org/doi/abs/10.1145/3551389">-->
<!--                    <span class="papertitle">Robust detection of small and dense objects in images from autonomous-->
<!--                      aerial vehicles</span>-->
<!--                  </a>-->
<!--                  <br>-->
<!--                  <b>Joo Chan Lee</b>, JeongYeop Yoo, Yongwoo Kim, SungTae Moon, Jong Hwan Ko-->
<!--                  <br>-->
<!--                  <em>EL</em>, 2021-->
<!--                  <br>-->
<!--                  <div>-->
<!--                    <a href="https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/ell2.12245">[Paper]</a>-->
<!--                  </div>-->
<!--                  <br>-->
<!--                  <p>-->
<!--                    Technical report for high performance small object detection.-->
<!--                    <br>-->
<!--                </td>-->
<!--              </tr>-->

<!--              <tr>-->
<!--                <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--                  <div class="one">-->
<!--                    <data-->
<!--                      src='https://iris.skku.edu/publication/c25_eccv_2020/Featured_hu84971219c3628f7f9d6df5f86721df13_187303_720x2500_fit_q75_h2_lanczos.webp'-->
<!--                      width="160" style='margin-top: 25px' ;>-->
<!--                  </div>-->
<!--                </td>-->
<!--                <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--                  <a href="https://dl.acm.org/doi/abs/10.1145/3551389">-->
<!--                    <span class="papertitle">VisDrone-DET2020: The Vision Meets Drone Object Detection in Image-->
<!--                      Challenge Results</span>-->
<!--                  </a>-->
<!--                  <br>-->
<!--                  Challenge Participants-->
<!--                  <br>-->
<!--                  <em>ECCV Workshops</em>, 2020-->
<!--                  <br>-->
<!--                  <div>-->
<!--                    <a href="https://link.springer.com/chapter/10.1007/978-3-030-66823-5_42">[Paper]</a>-->
<!--                    <a href="http://iris.skku.edu/img/2020-task1_first.png">[Certificate]</a>-->
<!--                  </div>-->
<!--                  <br>-->
<!--                  <p>-->
<!--                    VisDrone Challenge 1st Place Winner.-->
<!--                    <br>-->
<!--                </td>-->
<!--              </tr>-->

              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:0px">
                      <br>
                      <p style="text-align:center;font-size:small;">
                        Design and source code from <a href="https://jonbarron.info/">Jon Barron's website</a>.
                      </p>
                    </td>
                  </tr>
                </tbody>
              </table>
        </td>
      </tr>
  </table>
</body>

</html>