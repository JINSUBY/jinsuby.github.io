<!DOCTYPE html>
<html lang="en">

<head>
  <title>DomCLP: Domain-wise Contrastive Learning with Prototype Mixup for Unsupervised Domain Generalization</title>
  <meta charset="UTF-8">
  <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link href="css/style.css" rel="stylesheet" type="text/css" />
  <style>
    html,
    body,
    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
      font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    }

    < !-- .cite {
      background: #f0f0f0;
      padding: 10px;
      font-size: 18px
    }

    -->.cite {
      padding: 0px;
      background: #ffffff;
      font-size: 18px
    }

    .card {
      border: 1px solid #ccc
    }

    img {
      margin-bottom: -6px;
    }

    p {
      font-size: 18px;
    }

    a {
      text-decoration: none;
      color: #2196F3;
    }

    .bibtexsection {
      width: 100%;
      height: 20em;
      font-family: "Courier", monospace;
      /* font-size: 1vw; */
      white-space: pre;
      background-color: #f4f4f4;
      margin-left: auto;
      margin-right: auto;
      text-align: left;
    }

    .layered-paper-big {
      /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
      box-shadow:
        0px 0px 1px 1px rgba(0, 0, 0, 0.35),
        /* The top layer shadow */
        5px 5px 0 0px #fff,
        /* The second layer */
        5px 5px 1px 1px rgba(0, 0, 0, 0.35),
        /* The second layer shadow */
        10px 10px 0 0px #fff,
        /* The third layer */
        10px 10px 1px 1px rgba(0, 0, 0, 0.35),
        /* The third layer shadow */
        15px 15px 0 0px #fff,
        /* The fourth layer */
        15px 15px 1px 1px rgba(0, 0, 0, 0.35),
        /* The fourth layer shadow */
        20px 20px 0 0px #fff,
        /* The fifth layer */
        20px 20px 1px 1px rgba(0, 0, 0, 0.35),
        /* The fifth layer shadow */
        25px 25px 0 0px #fff,
        /* The fifth layer */
        25px 25px 1px 1px rgba(0, 0, 0, 0.35);
      /* The fifth layer shadow */
      margin-left: 10px;
      margin-right: 60px;
    }

    table {
      text-align: center;
      margin-left: auto;
      margin-right: auto;
      border-collapse: collapse;
    }

    table> :is(thead, tbody)>tr> :is(th, td) {
      padding: 3px;
      text-align: center;
    }

    table>thead>tr> :is(th, td) {
      border-top: 2px solid;
      border-bottom: 1px solid;
    }

    table>tbody>tr:last-child> :is(th, td) {
      border-bottom: 2px solid;
    }

    table>tbody {
      border-top: 2px solid;
      border-bottom: 1px solid;
    }
  </style>
</head>

<body class="w3-white">
  <!-- Page Container -->
  <div class="w3-content w3-margin-top w3-margin-bottom" style="max-width:960px;">

    <!-- The Grid -->
    <div class="w3-row-padding">

      <!-- paper container -->
      <div class="w3-display-container w3-row w3-white w3-margin-bottom">
        <!-- <div class="w3-center">
          <h1>Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields</h1> -->

          <!-- <div class="w3-center">
            <h4><b>Preprint</b> [<a href="https://github.com/maincold2/Dynamic_C3DGS/"><b>Code</b></a>] [<a href="https://arxiv.org/abs/2408.03822"><b>Paper</b></a>]</h4>
            </div> -->
            <div class="w3-center">
              <h1>DomCLP: Domain-wise Contrastive Learning with Prototype Mixup for Unsupervised Domain Generalization</h1>

              <div class="w3-center">
                <h4><b>AAAI 2025</b></h4>
                <h4>[<a href="https://arxiv.org/abs/2412.09074"><b>Paper</b></a>]
                  [<a href="https://github.com/JINSUBY/DomCLP"><b>Code</b></a>]</h4>
                </div>
                <h5><a href="https://jinsuby.github.io">Jin-Seop Lee</a>, Noo-ri Kim, Jee-Hyong Lee</h5>
                <h5>Sungkyunkwan University</h5>
              </div>

              <div class="w3-center">
                <h2>Abstract</h2>
              </div>
              <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                <embed src="images/figure1.pdf" style="width:100%">

              </div>
              <p align="justify">
                Self-supervised learning (SSL) methods based on the instance discrimination tasks with InfoNCE have achieved remarkable success.
                Despite their success, SSL models often struggle to generate effective representations for unseen-domain data.
                To address this issue, research on unsupervised domain generalization (UDG),
                which aims to develop SSL models that can generate domain-irrelevant features, has been conducted.
                Most UDG approaches utilize contrastive learning with InfoNCE to generate representations,
                and perform feature alignment based on strong assumptions to generalize domain-irrelevant common features from multi-source domains.
                However, existing methods that rely on instance discrimination tasks are not effective at extracting domain-irrelevant common features.
                This leads to the suppression of domain-irrelevant common features and the amplification of domain-relevant features,
                thereby hindering domain generalization. Furthermore, strong assumptions underlying feature alignment can lead to biased feature learning,
                reducing the diversity of common features.
                In this paper, we propose a novel approach, DomCLP, Domain-wise Contrastive Learning with Prototype Mixup.
                We explore how InfoNCE suppresses domain-irrelevant common features and amplifies domain-relevant features.
                Based on this analysis, we propose Domain-wise Contrastive Learning (\textit{DCon}) to enhance domain-irrelevant common features.
                We also propose Prototype Mixup Learning (\textit{PMix}) to generalize domain-irrelevant common features across multiple domains
                without relying on strong assumptions.
                The proposed method consistently outperforms state-of-the-art methods on the PACS and DomainNet datasets across various label fractions,
                showing significant improvements.
              </p>
              <hr>



              <hr>
              <!-- <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                <embed src="images/fig_demo.jpg" style="width:100%">

              </div>

              <hr>
              <div class="w3-center">
                <h2>Architecture Overview</h2> -->
              </div>
              <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                <embed src="images/fig_arch.jpg" style="width:100%">

              </div>
              <!-- <div class="w3-center">
                    <h2>Abstract</h2>
                </div> -->
              <p align="justify">
                3D Gaussian splatting (3DGS) has recently emerged as an alternative representation that leverages a 3D
                Gaussisan-based representation and adopts the rasterization pipeline to render the images rather than
                volumetric rendering, achieving very fast rendering speed and promising image quality.
                However, a significant drawback arises as 3DGS entails a substantial number of 3D Gaussians to maintain
                the high fidelity of the rendered images, which requires a large amount of memory and storage.
                To address this critical issue, we place a specific emphasis on two key objectives: reducing the number
                of Gaussian points without sacrificing performance and compressing the Gaussian attributes, such as
                view-dependent color and covariance.
                To this end, we propose a learnable mask strategy that significantly reduces the number of Gaussians
                while preserving high performance. In addition, we propose a compact but effective representation of
                view-dependent color by employing a grid-based neural field rather than relying on spherical harmonics.
                Finally, we learn codebooks to compactly represent the geometric attributes of Gaussian by vector
                quantization.
                With model compression techniques such as quantization and entropy coding, we consistently show over 25x
                reduced storage and enhanced rendering speed, while maintaining the quality of the scene representation,
                compared to 3DGS.
                Our work provides a comprehensive framework for 3D scene representation, achieving high performance,
                fast training, compactness, and real-time rendering.
              </p>

              <hr>

<!--
              <div class="w3-center">
                <h2>The effect of masking</h2>
                <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                  <embed src="images/fig_mask.jpg" style="width:45%">
                </div>
                <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                  <embed src="images/fig_vis_mask.jpg" style="width:100%">
                </div>
                <p>Masking can significantly reduce the number of Gaussians while retaining high quality.</p>
              </div>

              <hr>
              <div class="w3-center">
                <h2>The detailed process of R-VQ</h2>
              </div>
              <div class="w3-display-container w3-row w3-white w3-margin-bottom">
                <embed src="images/fig_rvq.jpg" style="width:100%">
                <p align="justify">In the first stage, the scale and rotation vectors are compared to codes in each
                  codebook, with the closest code identified as the result. In the next stage, the residual between the
                  original vector and the first stage's result is compared with another codebook. This process is
                  repeated up to the final stage.</p>
              </div>
              <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                <embed src="images/fig_vis_rvq.jpg" style="width:50%">
                <p align="justify">This reduces the storage requirements by approximately 30% while maintaining the
                  quality of reconstruction, training time, and rendering speed.</p>
              </div> -->

              <div class="w3-center">
                <h2>Results</h2>
                <p align="justify">
                  In addition to the end-to-end trained models (Ours), we implemented straightforward post-processing techniques
                  on the model attributes, a variant we denote as Ours+PP. These post-processing steps include:
                  1) Applying 8-bit min-max quantization to hash grid parameters and scalar attributes. 2) Pruning hash grid
                  parameters with values below 0.1.
                  3) Sorting Gaussians in Morton order.
                  4) Applying Huffman encoding on the 8-bit quantized values (hash parameters and scalar attributes) and R-VQ indices, and compressing the results using DEFLATE
              </div>
              <h3>Static scenes</h3>
              <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                <embed src="images/fig_qual_s.png" style="width:100%">
              </div>

              <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                <embed src="images/tab1.png" style="width:98%">
              </div>

              <div class="w3-center">
                <!-- <h2>Videos</h2> -->
                <div style="width: 49.5%; display: inline-block;">
                  <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                    <embed src="images/tab2.png" style="width:95%">
                  </div>
                </div>
                <div style="width: 49.5%; display: inline-block;">
                  <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                    <embed src="images/tab3.png" style="width:95%">
                  </div>
                </div>
              </div>

              <h3>Dynamic scenes</h3>

              <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                <embed src="images/fig_qual_d.png" style="width:100%">
              </div>

              <div class="w3-center">
                <!-- <h2>Videos</h2> -->
                <div style="width: 49.5%; display: inline-block;">
                  <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                    <embed src="images/tab4.png" style="width:95%">
                  </div>
                </div>
                <div style="width: 49.5%; display: inline-block;">
                  <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                    <embed src="images/tab5.png" style="width:95%">
                  </div>
                </div>
              </div>


</div>

<hr>
<div class="w3-center">
<h2>Bibtex</h2>
</div>
<textarea class="bibtexsection" readonly>
  @misc{lee2024domclpdomainwisecontrastivelearning,
      title={DomCLP: Domain-wise Contrastive Learning with Prototype Mixup for Unsupervised Domain Generalization},
      author={Jin-Seop Lee and Noo-ri Kim and Jee-Hyong Lee},
      year={2024},
      eprint={2412.09074},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.09074},
}
}</textarea>

            <hr>
            <div class="w3-center">
              <p class="text-justify">
                We used the project page of <a href="https://daniel03c1.github.io/masked_wavelet_nerf/">Masked Wavelet
                  NeRF</a> as a template.
              </p>
            </div>
          </div>
          <!-- end paper container -->

        </div><!-- End Grid -->
      </div><!-- End Page Container -->

</body>
<script defer src="https://unpkg.com/img-comparison-slider@7/dist/index.js"></script>
<script type="text/javascript" src="js/main.js"></script>
<link rel="stylesheet" href="https://unpkg.com/img-comparison-slider@7/dist/styles.css" />

</html>