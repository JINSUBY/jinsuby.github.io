<!DOCTYPE html>
<html lang="en">

<head>
  <title>DomCLP: Domain-wise Contrastive Learning with Prototype Mixup for Unsupervised Domain Generalization</title>
  <meta charset="UTF-8">
  <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link href="css/style.css" rel="stylesheet" type="text/css" />
  <style>
    html,
    body,
    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
      font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    }

    < !-- .cite {
      background: #f0f0f0;
      padding: 10px;
      font-size: 18px
    }

    -->.cite {
      padding: 0px;
      background: #ffffff;
      font-size: 18px
    }

    .card {
      border: 1px solid #ccc
    }

    img {
      margin-bottom: -6px;
    }

    p {
      font-size: 18px;
    }

    a {
      text-decoration: none;
      color: #2196F3;
    }

    .bibtexsection {
      width: 100%;
      height: 20em;
      font-family: "Courier", monospace;
      /* font-size: 1vw; */
      white-space: pre;
      background-color: #f4f4f4;
      margin-left: auto;
      margin-right: auto;
      text-align: left;
    }

    .layered-paper-big {
      /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
      box-shadow:
        0px 0px 1px 1px rgba(0, 0, 0, 0.35),
        /* The top layer shadow */
        5px 5px 0 0px #fff,
        /* The second layer */
        5px 5px 1px 1px rgba(0, 0, 0, 0.35),
        /* The second layer shadow */
        10px 10px 0 0px #fff,
        /* The third layer */
        10px 10px 1px 1px rgba(0, 0, 0, 0.35),
        /* The third layer shadow */
        15px 15px 0 0px #fff,
        /* The fourth layer */
        15px 15px 1px 1px rgba(0, 0, 0, 0.35),
        /* The fourth layer shadow */
        20px 20px 0 0px #fff,
        /* The fifth layer */
        20px 20px 1px 1px rgba(0, 0, 0, 0.35),
        /* The fifth layer shadow */
        25px 25px 0 0px #fff,
        /* The fifth layer */
        25px 25px 1px 1px rgba(0, 0, 0, 0.35);
      /* The fifth layer shadow */
      margin-left: 10px;
      margin-right: 60px;
    }

    table {
      text-align: center;
      margin-left: auto;
      margin-right: auto;
      border-collapse: collapse;
    }

    table> :is(thead, tbody)>tr> :is(th, td) {
      padding: 3px;
      text-align: center;
    }

    table>thead>tr> :is(th, td) {
      border-top: 2px solid;
      border-bottom: 1px solid;
    }

    table>tbody>tr:last-child> :is(th, td) {
      border-bottom: 2px solid;
    }

    table>tbody {
      border-top: 2px solid;
      border-bottom: 1px solid;
    }
  </style>
</head>

<body class="w3-white">
  <!-- Page Container -->
  <div class="w3-content w3-margin-top w3-margin-bottom" style="max-width:960px;">

    <!-- The Grid -->
    <div class="w3-row-padding">

      <!-- paper container -->
      <div class="w3-display-container w3-row w3-white w3-margin-bottom">
        <!-- <div class="w3-center">
          <h1>Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields</h1> -->

          <!-- <div class="w3-center">
            <h4><b>Preprint</b> [<a href="https://github.com/maincold2/Dynamic_C3DGS/"><b>Code</b></a>] [<a href="https://arxiv.org/abs/2408.03822"><b>Paper</b></a>]</h4>
            </div> -->
            <div class="w3-center">
              <h1>DomCLP: Domain-wise Contrastive Learning with Prototype Mixup for Unsupervised Domain Generalization</h1>

              <div class="w3-center">
                <h4><b>AAAI 2025</b></h4>
                <h4>[<a href="https://arxiv.org/abs/2412.09074"><b>Paper</b></a>]
                  [<a href="https://github.com/JINSUBY/DomCLP"><b>Code</b></a>]</h4>
                </div>
                <h5><a href="https://jinsuby.github.io">Jin-Seop Lee</a>, Noo-ri Kim, Jee-Hyong Lee</h5>
                <h5>Sungkyunkwan University</h5>
              </div>

              <div class="w3-center">
                <h2>Abstract</h2>
              </div>
              <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                <embed src="images/figure1.PNG" style="width:50%">
              </div>
              <p align="justify">
                Self-supervised learning (SSL) methods based on the instance discrimination tasks with InfoNCE have achieved remarkable success.
                Despite their success, SSL models often struggle to generate effective representations for unseen-domain data.
                To address this issue, research on unsupervised domain generalization (UDG),
                which aims to develop SSL models that can generate domain-irrelevant features, has been conducted.
                Most UDG approaches utilize contrastive learning with InfoNCE to generate representations,
                and perform feature alignment based on strong assumptions to generalize domain-irrelevant common features from multi-source domains.
                However, existing methods that rely on instance discrimination tasks are not effective at extracting domain-irrelevant common features.
                This leads to the suppression of domain-irrelevant common features and the amplification of domain-relevant features,
                thereby hindering domain generalization. Furthermore, strong assumptions underlying feature alignment can lead to biased feature learning,
                reducing the diversity of common features.
                In this paper, we propose a novel approach, DomCLP, Domain-wise Contrastive Learning with Prototype Mixup.
                We explore how InfoNCE suppresses domain-irrelevant common features and amplifies domain-relevant features.
                Based on this analysis, we propose Domain-wise Contrastive Learning (DCon) to enhance domain-irrelevant common features.
                We also propose Prototype Mixup Learning (PMix) to generalize domain-irrelevant common features across multiple domains
                without relying on strong assumptions.
                The proposed method consistently outperforms state-of-the-art methods on the PACS and DomainNet datasets across various label fractions,
                showing significant improvements.
              </p>
              <hr>

              <div class="w3-center">
                <h2>Limitations of Existing Method</h2>
              </div>
              <div class="w3-center">
                <!-- <h2>Videos</h2> -->
                <div style="width: 49.5%; display: inline-block;">
                  <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                    <embed src="images/figure2_1.png" style="width:95%">
                  </div>
                </div>
                <div style="width: 49.5%; display: inline-block;">
                  <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                    <embed src="images/figure2_2.png" style="width:95%">
                  </div>
                </div>
              </div>

              <p align="justify">
              Most UDG approaches rely on instance discrimination tasks, which are not well-suited for domain generalization.
              In contrastive learning with InfoNCE, representations are learned to distinguish between instances.
              If the model attempts to differentiate between instances, deep neural networks tend to learn features that are useful to discriminate instances.
              In UDG environments, they are easy to capture domain-relevant features rather than domain-irrelevant common features,
                because domain-relevant features are more helpful to distinguish instances across various domains.
              As a result, the instance discrimination task suppresses domain-irrelevant features and amplifies domain-relevant features, thereby hindering domain generalization.

              Another limitation is that previous approaches heavily depend on feature alignment strategies across multi-domain,
                which reduce the diversity of domain-irrelevant common features.
              </p>
              <hr>


              <div class="w3-center">
                <h2>Proposed Method</h2>
              </div>
              <div class="w3-center">
                <!-- <h2>Videos</h2> -->
                <div style="width: 49.5%; display: inline-block;">
                  <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                    <embed src="images/figure3_1.png" style="width:95%">
                  </div>
                </div>
                <div style="width: 49.5%; display: inline-block;">
                  <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                    <embed src="images/figure3_2.png" style="width:95%">
                  </div>
                </div>
              </div>

              <p align="justify">
              We theoretically explore how InfoNCE leads to suppress domain-irrelevant common features and amplify domain-relevant features.
              Based on this analysis, we propose the Domain-wise Contrastive Learning (DCon) to generate feature representations with enhanced domain-irrelevant common features.
              Furthermore, we present the Prototype Mixup Learning (PMix) to generalize diverse common features across multi-domain.
              </p>
              <hr>

              <div class="w3-center">
                <h2>Results</h2>

              </div>
              <h3>PACS & DomainNet dataset</h3>
              <div class="w3-center">
                <!-- <h2>Videos</h2> -->
                <div style="width: 49.5%; display: inline-block;">
                  <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                    <embed src="images/figure4_1.png" style="width:95%">
                  </div>
                </div>
                <div style="width: 49.5%; display: inline-block;">
                  <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                    <embed src="images/figure4_2.png" style="width:95%">
                  </div>
                </div>
              </div>

              <h3>T-SNE visualization</h3>
              <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                <embed src="images/figure4_3.png" style="width:50%">
              </div>

              <h3>Multi-Domain 3D Toy example</h3>
              <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
                <embed src="images/figure4_4.png" style="width:70%">
              </div>


</div>

<hr>
<div class="w3-center">
<h2>Bibtex</h2>
</div>
<textarea class="bibtexsection" readonly>
  @misc{lee2024domclpdomainwisecontrastivelearning,
      title={DomCLP: Domain-wise Contrastive Learning with Prototype Mixup for Unsupervised Domain Generalization},
      author={Jin-Seop Lee and Noo-ri Kim and Jee-Hyong Lee},
      year={2024},
      eprint={2412.09074},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.09074},
}
}</textarea>

            <hr>
            <div class="w3-center">
              <p class="text-justify">
                We used the project page of <a href="https://daniel03c1.github.io/masked_wavelet_nerf/">Masked Wavelet
                  NeRF</a> as a template.
              </p>
            </div>
          </div>
          <!-- end paper container -->

        </div><!-- End Grid -->
      </div><!-- End Page Container -->

</body>
<script defer src="https://unpkg.com/img-comparison-slider@7/dist/index.js"></script>
<script type="text/javascript" src="js/main.js"></script>
<link rel="stylesheet" href="https://unpkg.com/img-comparison-slider@7/dist/styles.css" />

</html>